{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f13d837-b2dd-4493-b2bb-0b0f5321e8f8",
   "metadata": {},
   "source": [
    "# Feature Extraction for Real-Time Drone Sound Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d0c09f-e425-4c00-a8a3-e67ed7c7e0f6",
   "metadata": {},
   "source": [
    "This notebook focuses on preparing the audio dataset for drone sound detection. It performs the following key steps:\n",
    "\n",
    " - **Loading the audio data** from the dataset.\n",
    "\n",
    " - **Extracting MFCC features** from each audio file to use as input for the model.\n",
    "\n",
    " - **Applying data augmentation** on samples labeled as \"drone\" to improve model robustness:\n",
    "\n",
    "     - Time stretching\n",
    "    \n",
    "     - Pitch shifting\n",
    "    \n",
    "     - Adding Gaussian noise\n",
    "    \n",
    "     - Scaling volume\n",
    "\n",
    " - **Saving the extracted features** in a compressed .npz file format for later use in the model development phase.\n",
    "\n",
    " - **Visualizing mel-spectrograms** for both drone and non-drone audio samples to better understand the data.\n",
    "\n",
    "This step is essential for transforming raw audio into a structured format suitable for training a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "200ca975-5ab0-4cd9-bfb5-c5db215c319b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Required Libraries\n",
    "\n",
    "# Standard library\n",
    "import os                                    # For handling file system paths and directory operations\n",
    "import random                                # For introducing randomness (e.g., in data augmentation)\n",
    "import warnings                              # To suppress unnecessary warnings\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np                           # For numerical operations and working with arrays\n",
    "import librosa                               # For audio processing (loading audio, extracting features, etc.)\n",
    "import librosa.display                       # For visualizing audio data like waveforms and spectrograms\n",
    "import matplotlib.pyplot as plt              # For plotting graphs and visualizing data\n",
    "from collections import Counter              # For counting elements, e.g., label frequencies\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0d4d303-2ee7-4f90-8b92-c2a57c2201d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting paths to the drone and non-drone audio subdirectories\n",
    "drone_path = \"../data/drone\"\n",
    "non_drone_path = \"../data/non_drone\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3afb659d-9771-4515-a8ed-a671493dd845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of raw audio files:\n",
      "Drone: 4988\n",
      "Non-Drone: 13806\n"
     ]
    }
   ],
   "source": [
    "# The number of .wav files in each category\n",
    "drone_files = [f for f in os.listdir(drone_path) if f.endswith(\".wav\")]\n",
    "non_drone_files = [f for f in os.listdir(non_drone_path) if f.endswith(\".wav\")]\n",
    "\n",
    "print(f\"Number of raw audio files:\\nDrone: {len(drone_files)}\\nNon-Drone: {len(non_drone_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc76c863-b4ec-483b-9ba0-dfc31df6938f",
   "metadata": {},
   "source": [
    "## Feature Extraction & Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc1a3ef3-f3a6-4107-bc8d-f910df8d315f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting audio processing parameters\n",
    "SAMPLE_RATE = 16000  # Target sample rate for all audio files\n",
    "N_MFCC = 13  # Number of MFCC features to extract\n",
    "FRAME_LENGTH = 1  # Frame length in seconds for segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d12f4da-256b-4d0b-90f1-c36526ca9312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting MFCC features from audio with optional data augmentation\n",
    "def extract_features(file_path, sample_rate=SAMPLE_RATE, augment=False):\n",
    "    \"\"\"\n",
    "    Extracts MFCC features from an audio file, optionally applying data augmentation.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the audio file.\n",
    "        sample_rate (int): Target sample rate for loading the audio.\n",
    "        augment (bool): Whether to apply data augmentation techniques.\n",
    "\n",
    "    Returns:\n",
    "        List[np.ndarray]: A list of 2D MFCC feature arrays (time_steps, n_mfcc).\n",
    "    \"\"\"\n",
    "    y, sr = librosa.load(file_path, sr=sample_rate)\n",
    "    segment_length = sample_rate * FRAME_LENGTH\n",
    "    segments = []\n",
    "\n",
    "    def get_mfcc(segment):\n",
    "        mfcc = librosa.feature.mfcc(y=segment, sr=sr, n_mfcc=N_MFCC)\n",
    "        return mfcc.T  # Transposing to shape (time_steps, n_mfcc)\n",
    "\n",
    "    # Splitting the audio into 1-second segments and extracting MFCCs\n",
    "    for i in range(0, len(y), segment_length):\n",
    "        segment = y[i:i + segment_length]\n",
    "        if len(segment) < segment_length:\n",
    "            continue\n",
    "        segments.append(get_mfcc(segment))\n",
    "\n",
    "        if augment:\n",
    "            # Applying time stretching\n",
    "            stretched = librosa.effects.time_stretch(segment, rate=np.random.uniform(0.8, 1.2))\n",
    "            if len(stretched) >= segment_length:\n",
    "                segments.append(get_mfcc(stretched[:segment_length]))\n",
    "\n",
    "            # Applying pitch shifting\n",
    "            pitched = librosa.effects.pitch_shift(segment, sr=sr, n_steps=np.random.randint(-2, 3))\n",
    "            segments.append(get_mfcc(pitched))\n",
    "\n",
    "            # Adding Gaussian noise\n",
    "            noise = 0.005 * np.random.randn(len(segment))\n",
    "            noisy = segment + noise\n",
    "            segments.append(get_mfcc(noisy))\n",
    "\n",
    "            # Scaling volume\n",
    "            scaled = segment * np.random.uniform(0.7, 1.3)\n",
    "            segments.append(get_mfcc(scaled))\n",
    "\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44800936-a54e-43d1-9bc2-fe04404a924f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data from both drone and non-drone folders, extracting features and labels\n",
    "def load_data(drone_path, non_drone_path):\n",
    "    \"\"\"\n",
    "    Loads audio files from drone and non-drone directories, applies feature extraction,\n",
    "    and returns features and labels.\n",
    "\n",
    "    Args:\n",
    "        drone_path (str): Path to the directory containing drone audio files.\n",
    "        non_drone_path (str): Path to the directory containing non-drone audio files.\n",
    "\n",
    "    Returns:\n",
    "        X (List[np.ndarray]): List of MFCC feature arrays.\n",
    "        y (List[int]): Corresponding list of labels (1 for drone, 0 for non-drone).\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "\n",
    "    # Processing drone audio files with augmentation\n",
    "    for file in os.listdir(drone_path):\n",
    "        if file.endswith(\".wav\"):\n",
    "            features = extract_features(os.path.join(drone_path, file), augment=True)\n",
    "            X.extend(features)\n",
    "            y.extend([1] * len(features))\n",
    "            assert len(X) == len(y), f\"Mismatch after {file}\"\n",
    "\n",
    "    # Processing non-drone audio files without augmentation\n",
    "    for file in os.listdir(non_drone_path):\n",
    "        if file.endswith(\".wav\"):\n",
    "            features = extract_features(os.path.join(non_drone_path, file), augment=False)\n",
    "            X.extend(features)\n",
    "            y.extend([0] * len(features))\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82e7a4b6-0545-4f1d-88a2-111d101e4f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "X, y = load_data(drone_path, non_drone_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5632d244-85de-43de-a7c4-c98c368f2795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Distribution:\n",
      "Drone: 22569\n",
      "Non-Drone: 11767\n"
     ]
    }
   ],
   "source": [
    "# Counting and printing the number of samples for each label (1 = Drone, 0 = Non-Drone)\n",
    "label_counts = Counter(y)\n",
    "print(f\"Label Distribution:\\nDrone: {label_counts[1]}\\nNon-Drone: {label_counts[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b87ad83-2171-442b-917c-c3f31c99c0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting X (list of 2D arrays) to an object array\n",
    "X_array = np.array(X, dtype=object)\n",
    "y_array = np.array(y)\n",
    "\n",
    "# Saving to file\n",
    "np.savez_compressed(\"audio_features.npz\", X=X_array, y=y_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d22b2fa-b2c1-4ea1-9f9c-f18db27e30f2",
   "metadata": {},
   "source": [
    "## Audio Label Counts Before and After Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f91f0e13-ae65-45e0-b76b-0e6bd95a1f3d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m aug_counts \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDrone\u001b[39m\u001b[38;5;124m'\u001b[39m: label_counts[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNon-Drone\u001b[39m\u001b[38;5;124m'\u001b[39m: label_counts[\u001b[38;5;241m0\u001b[39m]}\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Combining into DataFrame\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m combined_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRaw Data\u001b[39m\u001b[38;5;124m'\u001b[39m: raw_counts,\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAugmented Data\u001b[39m\u001b[38;5;124m'\u001b[39m: aug_counts\n\u001b[0;32m      9\u001b[0m })\n\u001b[0;32m     10\u001b[0m combined_df\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLabel\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Saving as CSV\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Preparing data for table\n",
    "raw_counts = {'Drone': len(drone_files), 'Non-Drone': len(non_drone_files)}\n",
    "aug_counts = {'Drone': label_counts[1], 'Non-Drone': label_counts[0]}\n",
    "\n",
    "# Combining into DataFrame\n",
    "combined_df = pd.DataFrame({\n",
    "    'Raw Data': raw_counts,\n",
    "    'Augmented Data': aug_counts\n",
    "})\n",
    "combined_df.index.name = 'Label'\n",
    "\n",
    "# Saving as CSV\n",
    "combined_df.to_csv('../results/tables/audio_label_counts_summary.csv')\n",
    "\n",
    "# Saving as LaTeX\n",
    "latex_code = combined_df.to_latex(\n",
    "    float_format='%.0f',\n",
    "    caption='Number of audio samples per label before and after augmentation',\n",
    "    label='tab:audio_label_counts'\n",
    ")\n",
    "with open('../results/tables/audio_label_counts_summary.tex', 'w') as f:\n",
    "    f.write(latex_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15638631-e742-494e-8421-e8500f88eb9a",
   "metadata": {},
   "source": [
    "## Visualizing Sample Mel-Spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a67549d-0305-41da-94b1-d693fe48406a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Randomly selecting 4 files from each class\n",
    "sample_drone_files = random.sample(drone_files, 4)\n",
    "sample_non_drone_files = random.sample(non_drone_files, 4)\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 6))\n",
    "fig.patch.set_facecolor('white')\n",
    "\n",
    "# Defining time tick positions and labels\n",
    "time_ticks = [0, 0.15, 0.3, 0.45, 0.6, 0.75, 0.9]\n",
    "time_labels = [f\"{t:.2f}\" for t in time_ticks]\n",
    "\n",
    "# Plotting Drone Samples\n",
    "for idx, file_path in enumerate(sample_drone_files):\n",
    "    y, sr = librosa.load(os.path.join(drone_path, file_path), sr=SAMPLE_RATE)\n",
    "    mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
    "    mel_db = librosa.power_to_db(mel, ref=np.max)\n",
    "\n",
    "    ax = axes[idx // 2][idx % 2]\n",
    "    librosa.display.specshow(mel_db, sr=sr, x_axis='time', y_axis='mel', ax=ax)\n",
    "    ax.set_title(f\"Drone {idx+1}\")\n",
    "    ax.set_xticks(time_ticks)\n",
    "    ax.set_xticklabels(time_labels)\n",
    "    ax.label_outer()\n",
    "\n",
    "# Plotting Non-Drone Samples\n",
    "for idx, file_path in enumerate(sample_non_drone_files):\n",
    "    y, sr = librosa.load(os.path.join(non_drone_path, file_path), sr=SAMPLE_RATE)\n",
    "    mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
    "    mel_db = librosa.power_to_db(mel, ref=np.max)\n",
    "\n",
    "    ax = axes[idx // 2][(idx % 2) + 2]\n",
    "    librosa.display.specshow(mel_db, sr=sr, x_axis='time', y_axis='mel', ax=ax)\n",
    "    ax.set_title(f\"Non-Drone {idx+1}\")\n",
    "    ax.set_xticks(time_ticks)\n",
    "    ax.set_xticklabels(time_labels)\n",
    "    ax.label_outer()\n",
    "\n",
    "# Adding group titles\n",
    "fig.text(0.25, 0.95, \"Mel-Spectrograms of Drone Audio Samples\", ha='center', fontsize=14, weight='bold')\n",
    "fig.text(0.75, 0.95, \"Mel-Spectrograms of Non-Drone Audio Samples\", ha='center', fontsize=14, weight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.88)\n",
    "\n",
    "# Saving the figure\n",
    "plt.savefig(\"../results/plots/mel_spectrogram_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
